\documentclass[10pt,a4paper]{book}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\title{math & ml notes}
\begin{document}



\chapter{Linear models}
\section{Problem Description}

Input data \textbf{x} N*M matrix, N data points, each has M dimension, or M features
\\
Target value \textbf{t}
\\
predict value \textbf{y(x)}
\\
Sometimes we use input transformation, to replace x with $\phi(x)$ (basis function), a N*D matrix, also called design matrix $\Phi$. Design matrix is often fixed non-linear functions of input variables, since it would be not necessary if it's linear(parameter vector w can cover this condition). 
\\



Candidate of basis function
\begin{itemize}
	\item powers of x, $x^j$
	\item Gaussian basis function, $\phi(x) = exp\{- \frac {(x-\mu_j)^2} {2s^2}\}$, where $\mu_j$ governs the location, and $s$ governs the space scale.
	\item logistic sigmoidal $\phi_j(x)=\sigma(\frac {x-\mu_j} s)$, where $\sigma(a) = \frac 1 {1+exp(-a)}$
	\item tanh $tanh(x) = \frac {e^x - e^{-x}} {e^x + e^{-x}}$ = $2\sigma(2x)-1$
	\item fourier basis (warelet)
\end{itemize}

\section{Least squares}
model
$$y = w^Tx$$

error function (squared error)
$$C(x) = (w^T\Phi - t)^T(w^T\Phi - t)$$

gradient w.r.t w
$$\triangledown C = \Phi^T(\Phi w -t)$$

let it be 0, we get 
$$w = (\Phi^T\Phi)^{-1}\Phi^Tt$$
(we need to investigate 2nd order derivative matrix to make sure this zero point is not a saddle point.)
\\
\\
to predict a test data
$$y' = w^Tx'$$

\section{Maximum likelihood}
all data points are assumed i.i.d Gaussian, $t = y(x,w)+\epsilon$ where $\epsilon$ is the zero mean Gaussian additive noise.

likelihood function
$$p(t|X,w,\beta) = \prod_n N(t_n|w^t\phi(x_n), \beta^{-1}) $$

maximize the likelihood, or the logarithm of the likelihood, is equal to minimize the sum of squared error, and at last get the same result as the least squares.

(need proof, 05Dec2017)If assume other density distribution, we get different model. e.g. for Laplacian noise, $p(\epsilon) = {\frac 1 {2a}} e^{-\frac {|z|} {a}}$, we get model $\hat{x} = argmin_x |ax-y|_1$

\section{Bayesian}
According to Bayesian formula:
$$ p(w|X,t) = \frac {p(t|X,w,\beta) * p(w)} {\sum_\beta  p(t|X,w,\beta) * p(w)} $$

$$ p(w|X,t) \propto  p(t|X,w,\beta) * p(w) $$

Given that $p(w|\alpha) = N(0,\alpha I)$ (isotropic Gaussian with zero mean), maximize the above function is same as minimize $|y-t|^2 + \alpha w^Tw$. This is squared errors plus ridge regularization

The first part is same as in MLE, and the second part is prior of w, with maximum log probability, the two parts added together as the cost, as in ridge regression.

Q: how to interpret lasso regression with Bayesian theory?
A: 17Dec2017: Laplace distribution is defined as
$$f(w|\mu, b) = {1 \over {2b}} \exp (- {|w-\mu| \over b})$$
$$\log f(w|\mu, b) = \log {1 \over {2b}} - {|w-\mu| \over b}$$

take the logarithm of both sides, we get $(- {|x-\mu| \over b})$, let $\mu = 0$ we get $|x|$/b, which is the regularization part of lasso regression.

\section {LDA}

It's said, that LDA is equivalent to liner regression for two classes problems.

$$ P(Y=k|X=x) = \frac {P(X=x|y=k) * P(y=k)}  {\Sigma_l P(X=x|y=l) * P(y=l) }$$
prior probability: $P(Y=k)$   
suppose $P(X=x|y=l)$ has normal distribution, and has equal variance for all $l$. then, to get the largest value of $P(Y=k|X=x)$, we need only to get the largest nominator in the right part.  
let $\pi_k = P(y=k)$
the formula above have same denominator for each class k, we need only compute the nominator. plug in the normal distribution formula, we get
$$ d_k(x) = \pi_k{\frac 1 {\sqrt{2\pi} \sigma }} exp({-{\frac 1 {2 \sigma^2}}(x - \mu_k)^2}) $$  
taking the log we get:  
$$log(d_k(x)) = log(\pi_k) -{1 \over 2 \sigma^2}(x - \mu_k)^2 - log(\sqrt{2\pi} \sigma) $$  
the last item is the same for all k, so we can ignore it also.  
and for given x, $x^2 \over {2 \sigma^2}$ is also a constant, at last we get:  
$$log(\pi_k) + {\mu_k x \over \sigma^2} - {\mu_k^2 \over 2 \sigma^2}$$
we can assign  x the class k for which the above formula is largest.


\section{Soft-max}

softmax is also called normalized exponential.
$$\mu_k = {exp(\eta_k) \over {1 + \sum_j exp(\eta_j)}}$$

some author doesn't use +1 version. the denominator is just the sum of exponential. Adding 1 is to protect it from being 0.



\section{Kernel methods}
regularized sum of squares, 
$$ J(w) = \frac 1 2 (\Phi w - y)^T(\Phi w -y) + \lambda w^Tw$$
gradient
$$\triangledown J = \Phi^T (\Phi w - y) +\lambda w$$

Let the gradient be 0, we get

$$w = \Phi^T a$$
where
$$a = - \frac 1 \lambda (\Phi w - y)$$

replace $\Phi^T a$ into J(w), we get

$$J(w) = \frac 1 2 (w^T\Phi^T\Phi w -y^T\Phi w -w^T\Phi^Ty +y^TY) + \lambda w^Tw$$
$$J(a) = \frac 1 2 (a^T\Phi\Phi^T\Phi \Phi^T a -y^T\Phi \Phi^T a -a^T \ Phi \Phi^Ty +y^Ty) + \lambda a^T\Phi \Phi^T a$$

let $$K= \Phi \Phi^T$$, we get

$$J(a) = \frac 1 2 (a^T K K a -y^T K a -a^T K^Ty +y^Ty) + \lambda a^T K a$$

$$\triangledown J(a) = KKa - K y +2 \lambda K a $$

let it be 0, we get

$$a = (K + 2\lambda I)^{-1}y$$

then to predict for x, we use

$$\hat y(x) = \phi(x) \Phi^T a = \phi(x) \Phi^T (K + 2 \lambda I)^{-1}y = k(x)^T (K + 2 \lambda I)^{-1}y$$

where $\phi(x)$ is a row vector, and K is called kernel matrix.


Although $k(x) = \phi(x) \Phi$ and $K = \Phi \Phi^T$, but in computation, we don't really need $\Phi$, we can design kernel functions directly, e.g. $k(x_m,x_n) = f(x_m, x_n) = \phi(x_m)^T \phi(x_n)$. 

The function must be a valid kernel, or, in other words, it must correspond to a scalar product in some feature space. Sometimes we can construct kernel functions corresponding to infinite dimensional feature spaces.

A commonly used kernel is Gaussian kernel:
 $$ k(x,x') = exp(\frac {- \vert x - x' \vert ^2} {2 \sigma ^2})$$

Problem: 
\begin{itemize}

	\item  how to train this model? or not to train at all? 

	A: reasoning about the process of inducing the prediction formula, we get the value of a by minimizing J(a), so we don't need to train the model again, it's already the best. But still we can adjust $\lambda$ or kernel functions, and test them with validation sets.

	Also in regularized linear regression model, these is a penalty parameter $\lambda$, it's chosen too, not trained.

	\item the prediction formula uses only one unknown variable $\lambda$, how to select this value?

	A: ref previous problem.

\end{itemize}

\subsection{Gaussian Process}

Assume w is isotropic Gaussian, $y=\Phi w, t = y + \epsilon$,y is linear combination of w, $\epsilon$ is also Gaussian independent of w, then y is also Gaussian.
 let
 $$P(w) = N(0, \alpha I)$$
 $$P(\epsilon) = N(0, \beta)$$
 then 

$$
    E(y) = \Phi E(w) = 0\\
$$
$$
  Cov(y) = E(y y^T) 
         = E(\Phi w w^T \Phi^T) 
         = \Phi E(w w^T) \Phi^T 
         = \alpha \Phi \Phi^T
         = K
$$

$$ P(y) = N(0, K) $$
$$ P(t|y) = N(0, \beta^{-1} I) $$
$$   P(t) = N(t|0, C) = N(t|0, K+ \beta ^{-1}) $$

the co-variance matrix of P(t) can be calculated by kernel functions, without using design matrix $\Phi$, this make it possible to handle infinite dimensional feature space.

Problems:
\begin{itemize}
	
	\item How to predict with Gaussian process model?

	A:use conditional Gaussian distribution formula:
 

	\item How to train a Gaussian Process model? what  parameters do we need to find?
	
	\item It's said that GP is modeling directly on functions space, which function?
\end{itemize}

\chapter{Algorithms}

\chapter{Statistical Learning}

\section {model metrics}
\begin{itemize}
	\item logloss
	\item AUC, area under curve (ROC curve)
	\item accuracy
	\item precision and recall
	
	precision: fraction of detections that were reported by the model that were correct.
	
	recall: fraction of true events that were detected.
	
	\item F1 score 
	$$ F1 = {2pr\over {p+r}} $$
	\item coverage
	
	fractions of examples that a machine learning system can produce a response. (the rest of which need a human being to decide.)
\end{itemize}

\subsection{regression metrics}
\begin{itemize}
	\item 
\end{itemize}
\subsection{classification metrics}
\begin{itemize}
	\item Accuracy score
	\item Cohen's kappa
	
	$$ \kappa = {{\rho_o - \rho_e} \over {1 - \rho_e}}$$
	where $\rho_o$ is relative observed agreement among raters (identical to accuracy), and $\rho_e$ is hypothetical probability of chance agreementt.
	
	$$\rho_e = {1\over N^2 }\sum_k n_{k1} n_{k2}$$
	
	where N is the number of items, $n_{k1}$ is number of items rater 1 give category k.
	
	\item confusion matrix
	A matrix that gives prediction pricision and recall rate for each class. usually used in multi-target object classification.
	A perfect confusion matrix is the one with elements all being zero except in the main diagonal.
	
	this matrix can give information about the how the model is confusing about clasification results.
	
	\item Hamming loss
	
	ratio of different label.
	$$L = {1\over N}\sum 1(y \neq \hat{y}) $$
	
	\item Jaccard similarity coefficient score
	
	Jaccard similarity coefficient of the i-th samples is
	$$J(y,\hat{y}) = { { |y \cap \hat{y} | } \over {|y \cup \hat{y}|}}$$
	
	\item $F_1$
	\item $F_{\beta}$
	
\end{itemize}

\section{Statistical concept}
\begin{itemize}
\item
population regression line

best linear approximation to the true model.
\item least square line

\item bias vs variance

$MSE = BIAS^2 + VARIANCE$
$$bias = E[\hat{y} - y]$$
$$variance = [\hat{y} - E[\hat{y}]]^2$$


$\hat{y}$ is estimator of y, consider it as a random variable, we can calculate its variance. [youtube, statistics for applications] named this MSE as estimator risk (or quadratic risk), and defined as $MSE = E[|\hat{y} - y|^2] = E[|\hat{y} - E[\hat{y}] + E[\hat{y}] - y|^2]$


In prml ,the author says that bias-variance trade-off is the product of frequentist viewpoint, and the maximum likelihood method's property, if we marginalize over parameters in a Bayesian setting this doesn't arise.





\item formula of standard error(SE)
$$ Var(\mu) = SE(\mu)^2 = \frac {\sigma^2} {n} $$

\item t-statistic, t-distribution (TBD)

\item degrees of freedom - number of free (independent) variables

\item f-statistic
\item F1-score
\item RSE - Residual Standard Error
$$RSE = \sqrt{{1 \over {n-2}} RSS}$$
\item $R^2$   
$$R^2 = {\frac {TSS - RSS} {TSS}}$$
TSS total sum of squares. 
$$TSS = \Sigma_i(y_i - E(y))^2$$  
RSS residual sum of squares. 
$$RSS = \Sigma(y_i - \hat{y})^2$$

R squared is the proportion of the variance.

(05Dec2017)$R^2$ has 2 different definition. Regression sum of squares, defined as $\sum(\hat{y} - E(y))^2$. Find out more details.

\item p-value  
\item null hypothesis (H) - assume null hypothesis, compute p-value, which shows the probability that the data is observed given that H is true. this is to say, p = P(D|H), the posterior probability? refer to chapter 37 of [1] and chapter 3 of [2]

\item alternative hypothesis

\item LDA  

\end{itemize}

\section {neural networks}
\begin{itemize}
\item sigmoid function  
$f(z) = {1 \over {1 + e^{-z}}}$  
why do we use -z other than z? Maybe to make f(z) a ascending function.
\end{itemize}

\section {least angle regression (LAR)}
https://en.wikipedia.org/wiki/Least-angle\_regression

\section {least absolute shrinkage and selection operator (LASSO)}

https://en.wikipedia.org/wiki/Least-angle\_regression

\section{spline functions}
\begin{itemize}


\item make a comparison between Gradient descent and Neuton method   
\item gradient descent, if we adjust parameters each time after one sample training, it's called stochastic gradient descent, and if we adjust parameters once after a batch training, it's called gradient descent.   
+ stochastic gradient descent, on-line learning   
+ gradient descent, batch learning.    
problem: why it's called stochastic?
problem2: some books call on-line learning those methods which test a few examples, not just limited to only one.   

\item check if the lasso regularization method finds the same model as SVD or PCA. 3Mar2006: svd and pca find potential dimensions (linearly combinations of existing ones), they do base-transformation, so apparently get different answer from lasso? and if so, check if other dimension-reducing methods get same answer as lasso.
\end{itemize}


\section{Bootstrap}
Original data points $X=\{x_1,x_2,...,x_N\}$

Create new data set by drawing N points at random from X, with replacement (some x may be replicated, and others may be absent). this process can be repeated L times, and we got L data sets.
\section {Perceptron}
ESL,4.5
compute a linear combination of input features and return the sign, is called perceptron.

It's linear, so the separator contains 0 point. $y=sign(w^Tx)$

How to train?

For mis-classified sample $x_i$, let $w \leftarrow w + y_i \eta x_i$. Repeat it until converged. 

\section {Random forest}
\section {Extra trees}
\section {Gradient Boost machine}

\section {Kernel methods}

\section{Gaussian process}

Linear regression in GP's view
\\
Linear classification in GP's view
\\
In normal Gaussian process, the index set is time series. but in machine learning context, the index set is often the index of input data.

\begin{itemize}
	\item how GP model make a prediction?
\end{itemize}

\section {Ensembling learning}
\subsection{Ensembing methods}
\begin{itemize}
	\item vote
	\item weighted vote
	\item averaging
	\item geometric mean (maybe better than averaging)
	\item rank averaging
	
	 Rank the predictions first, then averaging ranks. 
	\item stacked generalization
	
	 Use a pool of base classifiers, and then use another classifier to combine their predictions, aiming to reduce generalization error.
	\item blending
	
	almost same as stacked generalization. 
	
\end{itemize}
\subsection{reference}
	kaggle ensembling guide, mlwave.com

	
\section {time series}

for single variable time series,

\textbf{weak stationary}, means the auto-covariance depends on the separation of $x_s$ and $x_t$, say, $\vert s-t \vert$, and not on where the points are located in time, and the mean value function is constant.

\textbf{strictly stationary}, means the probabilistic behavior of every collection of values $\{x_{t1}, x_{t2}, ..., x_{tn}\}$ is identical to a time shift of the values $\{x_{t1+h}, x_{t2+h}, ..., x_{tn+h}\}$

for k-dimensional time series

\textbf{weakly stationary} if $E(z_t) = \mu$, a K-dimensional constant vector, and $Cov(z_t) = E((z_t-\mu)(z_t-\mu)^T) = \Sigma_z$, a constant $K \times K$ positive-definite matrix. in other words, the first two moments of Zt is time-invariant.

\textbf{strictly stationary} has the same meaning as for single-variable time series. the joint probability distribution of m collection, $(z_{t1}, z_{tm})$, is the same as that of $(z_{t1+j}, z_{tm+j})$

\section{Mixture Gaussian}
\section{EM}

  
\section{references}
[1] information theory, inference, and learning algorithms by David J.C. MacKay    
[2] statistical foundation of machine learning by Gianluca Bontempi  

\chapter{Discretization}
\section {Bayesian Discretization}
The basic idea is to use K-means algorithm to cluster values of an attribute, while the user has to specify the K value.
\section {ID3}
use information entropy to choose attributes and cutpoint.
$$H(S) = - \sum_{x \in S} p(x)log_2 {p(x)} $$
\section {C4.5}

\section {Maximum Marginal Entropy}

\section {Ent-MDLP (minimum description length principle)}
basic idea: a cutpoint for a set of points is accepted if the cost or length of the message required to send after partition is less than the cost or length of the message required to send before the partition.

\section {$ \chi^2 $}
\chapter{Optimization}
 Newton method
 
 BFGS
 
 Q:why in convex optimization problem the contraints functions need to be all convex?
\section {Line Search}

$$x_{k+1} = x_k + t_k \eta_k$$
where $\eta_k$ is the search direction, and $t_k$ is the step size.

\begin{itemize}
\item Scale Invariance

if a direction and step length are selected base on an algorithm that is not sersitive to x, it's called scale invariant.

\item Armijo condition
$$ f(x_k+\alpha p_k) <= f(x_k) + c_1 \alpha \triangledown f_k^T p_k $$
where $ \triangledown f_k^T p_k $ is the directional derivative. Armijo condition make sure that the decrease of f is proportional to both a and directional derivative. $c_1 \in (0, 1)$. But Armijo condition can't assure the sufficiency of decrease since for small $\alpha$ near $x_k$ the condition is almost always met.

\item Wolfe conditions
$$ \triangledown f(x_k+\alpha_k p_k)^T p_k >= c_2 \triangledown f_k^Tp_k $$
Wolfe conditions makes sure that the step length is not too small, as an additional condition to Armijo condition. It means the directional derivative at $\alpha_k$ is not too low, since low value means in this direction the value of f will decrease quickly and we may not need to stop here.

\item Strong Wolfe conditions

\item Goldstein conditions

\item Backtracking line search

just give a big stop lenght, and shrink it slowly. then we can avoid problem of Armijo condition.

\end{itemize}

\section natural gradient
\section SGD
\section AMS

\chapter{Statistics}

\section{Inference}
inference problems can be classified into 3 types: estimation, confidence sets, hypothesis testing.

\subsection{point estimation}
\begin{itemize}
\item point estimation\\
point estimation means give a single best guess of some quantity of interest, i.e. a parameter, a CDF, a PDF, a expected value of random variable. 


Estimation of a parameter should converge to true value if we collect more and more data. this is called consistent.


point estimation is often contrasted with the interval estimation, i.e. confidence interval in frequentiest inference, or credible intervals in Bayesian inference.

\item sampling distribution\\
distribution of a statistic, considered as a random variable. the name of sampling distribution comes from the fact that it depends on a random sample, and so its distribution is derived from the distribution of the sample.

sampling distributions is commonly used by those statisticians who prefer not to use prior and posterior distributions.

\item standard error\\
standard error is the standard deviation of the sampling distribution. the se of mean of N normal-distributed random variables is $\sigma \over \sqrt{N}$


\end{itemize}

\subsubsection {Method of moments estimator}
 this method finds the value of $\theta$ such that all moments of the distribution parameterized by this $\theta$ equal to corresponding sample moments. 
 $$ \int x^j dF_{\hat{\theta}}(x)  = {1 \over n}\sum_{i=1}^n x_i^j $$

 why we use this method? what is the rationale behind it?
 
 we believe if for any function h(x), $\int h(x)f_\theta(x)dx = \int h(x) f_{\theta^*}(x)dx  = E_{\theta^*}[h(x)]$, then $f_\theta(x)$ is a good approximation of  $f_{\theta^*}(x)$. But obviously we cannot test all possible $h(x)$, so we use only polynomials  (according to \textbf{Weiersstress approximation theorem}, any continuous function can be arbitrarily well approximated by polynomials), and we choose $x^k$ because any polynormials can be linearly combined by  $x^k$ where $ (k \in N)$. The last problem is what the upper bound of $k$ should we choose. as a rule of thumb, if $\theta \in R^d$, we need d moments.
 
 For discrete distributions, say, if there are $r$ possible values, $E=\{x_1,x_2,...x_r\}$, the PMF has $r-1$ unknowns, we could solve PMF with r-1 moment equations and $\sum p(x_i)=1$, this is multivariate linear problem. Written in matrix form we get a vandermonde matrix, which is invertible, and then we are assured a solution. (the determinant of vandermonde matrix is $\sum_{1 \le i < j \le r}(x_i-x_j)$, since x are possible values of a random variable, they are different by definition, so the determinant is non zero.) Note that this means for any distribution, we can use method of moments to determine its PMF (without any parameter), just by inverting a Vandermonde matrix. so in most cases, if we use parametric model, we need much less moments to estimate the parameters. And also note that here comes the numerical skills, if the inverse of the Vandermonde matrix has large \textbf{condition number}, then the estimated parameter $\theta$ has high variance,i.e. not reliable. The condition number is the largest eigen value of the matrix.
 
 in one word, if we can accurately estimate moments, then we may be able to recover pdf or pmf, So, how do we estimate the k-th moment? Given n sample, $\hat{m}_k= (1/n)\sum_{i=1}^n x_i^k$, this is called empirical moments.

\subsubsection {MLE}

MLE is introduced by R.A.Fisher in 1912.

minimize the total variant distance when we build an estimator is the ultimate target, but it is hard if not impossible to compute it, so we use Kullback-Leibler divergence, which has some good properties for our purpose. note that total variant distance is truly a distance metric, while KL is not (so it is called divergence). 


If we want to estimate the parameter $\theta^*$ of one distribution P, denoted as $\hat \theta$, the more close of the two distributions $P_\theta^*$ and $P_{\hat{\theta}}$ the better. So how do we measure the closeness of two distributions? total variant distance, 

$$TV(P_{\theta^*}, P_{\hat{\theta}}) = \max_A{|P_{\theta^*}(A) - P_{\hat{\theta}}(A)|}$$ 
where A represent some event. our strategy is to construct an estimator $$\hat{TV}(P_\theta, P_{\theta^*})$$
and then find $$\arg\min_\theta [\hat{TV}(P_\theta, P_{\theta^*})]$$

Now the problem is we don't know how to construct this expression, wo don't know $\theta^*$, and the space of A is so much big that we don't know how to iterate it neither. so we resort to KL divergence. KL is not a distance metric, but when $KL(P,Q) \to 0$ $Q \to P$. our estimator is now find $\theta$ of $$\min_\theta[KL(P_{\theta^*}, P_\theta)]$$

insert KL into formula above
$$
KL(P_{\theta^*}, P_\theta) = E_{\theta^*} [\log{P_{\theta^*}(x) \over P_{\theta}(x)} ] = E_{\theta^*}[\log{P_{\theta^*}(x) ] - E_{\theta^*}[\log P_{\theta}(x)}]
$$

the first part is constant, and the second is an expectation that we could extimate!

$$ KL(P_{\theta^*}, P_\theta)  \approx  Constant -  {1 \over n} \sum_{i=1}^N \log P_{\theta}(x_i)$$

we need only compute the maximum value of the second part.

$$
\arg\min_\theta \hat{KL}(P_{\theta^*}, P_\theta)  = \arg\max_\theta \sum_{i=1}^N \log P_{\theta}(x_i) =  \arg\max_\theta \prod_{i=1}^N P_{\theta}(x_i)
$$
bingo, you got MLE

since the second part of KL is cross entropy, we actually did minimization of cross entropy of two distributions.

last step: $\arg\max (\log a+\log b) = \arg\max \log ab = \arg\max ab$




For MLE, fisher information gives how sensitive the estimator is to change of $\theta$ (to be verified). and the variance of first derivative equals to minus expectation of second derivative. (to be verified also, and why is it useful?)




Properties of MLE:
\begin{list}{.}{ }
	\item it is consistent
	\item it is equivariant
	\item it is asymptotically normal
	\item it is asymptotically optimal or efficient
	\item it is approximately the Bayes estimator	
\end{list}

Problem: when does MEL work well? and when not? When are the properties above hold?


\subsection{confidence set}

\begin{itemize}


\item confidence interval\\

interval that contains an unknown quantity with given frequency	\\
1) definition\\
2) how is it computed\\
3) how is it implemented in R (algorithms e.g. LR in R gives confidence intervals by default)


\item {z-table, z-score}\\
for standard normal distribution curve, a z-score is a value in x-axis, z-table gives area under the curve between any two z-scores.


\item {normal-based confidence interval}\\
suppose $\hat{\theta}_n \approx N(\theta, \hat{se}^2) $
Let $$C_n = (\hat{\theta}_n - z_{\alpha / 2} \hat{se},  \hat{\theta}_n + z_{\alpha / 2} \hat{se})$$
then $$P(\theta \in C_n) \rightarrow 1-\alpha$$


\item {Normal-based confidence interval calculation}\\
https://www.wikihow.com/Calculate-Confidence-Interval
$$ \overline{x} \pm z_{\alpha / 2} * {\sigma \over \sqrt{N}}$$

where $\alpha$ is confidence level, i.e. $95\%$, $z_{\alpha / 2}$ is critical value, N is sample number. $\sigma$ is standard deviation of your target statistics.

\end{itemize}
\subsection{hypothesis testing}

Formally if we partition the parameter space into two subspaces, $\Theta_0$ and $\Theta_1$, \textbf{Null hypothesis} is $\theta \in \Theta_0$, the \textbf{Alternative hypothesis} is $\theta \in \Theta_1$. we define a \textbf{reject region} R, if $x \in R$ we reject the Null hypothesis, otherwise we don't reject it. usually, $$R = \{x : T(x) > c\} $$ where T is a \textbf{test statistic}, and c is \textbf{critical value}. 

Some sources define rejection region other way, e.g. a region of T(x) not of x.


Rejecting Null hypothesis when Null hypothesis is true is called \textbf{type I error (false positive)}. Retaining Null hypothesis when alternative hypothesis is true is called \textbf{type II error (false negative)}.


\textbf{power function} of a test is defined as $$\beta(\theta) = P_{\theta}(X \in R) $$, means the probability when X is in rejecting region under some value of $\theta$.  the \textbf{size} of a test is defined as the largest power function value when $H_0$ is true, i.e. $$\alpha = \sup_{\theta \in \Theta_0} \beta(\theta)$$


\subsubsection{Wald test}
testing $H_0: \theta = \theta_0$, $H_1: \theta \neq \theta_0$ and assuming $\hat{\theta}$ is asympotically normal: $$ {{\hat{\theta} - \theta_0} \over {\hat{se}}} \asymp N(0,1)  $$


$$W =  {{\hat{\theta} - \theta_0} \over {\hat{se}}}$$
	reject $H_0$ if $|W| > z_{\alpha /2}$

\subsubsection{$\chi^2$ test}
\subsubsection{permutation test}
\subsubsection{likelihood ratio test}

\subsection{Sampling distributions}
\subsubsection{chi-square ($\chi^2$) distribution}

it's a particular class of Gamma distribution.

\subsubsection{likelihood ratio test}

\section{Concepts}
\begin{itemize}
	
	
	
	\item set estimation
	
	\item confidence set
	\item sufficient statistics

\end{itemize}


\section {parameter inference}

\subsection {methods}
\begin{itemize}
	\item moments. why called moments?
	\item Maximum likelihood
	\item bayesian inference
\end{itemize}

\subsection {check assumption}
goodness-of-fit test


\section {Monte Carlo statistics}

\subsection {Importance sampling}
to estimate expectation of form
$$E(f) = \int f(z)p(z)dz$$
where p(z) is difficult to sample from, but easy to evaluate when z if given. we can estimate E(f) by sampling from  another distribution q(z), and adjust weights of each sample:
\begin{align}
E(f) &= \int f(z)p(z)dz\\
&= \int f(z){p(z) \over q(z)} q(z) dz \\
&= E_q[f(z) {p(z) \over q(z)}] \\
&\approx {1 \over L} \sum_{l=0}^L f(z^{(l)}){p(z^{(l)}) \over q(z^{(l)})}
\end{align}

$p(z) / q(z)$ is called likelihood ratio. (note that it is different from the one in 'likelihood ratio policy gradient' in RL)

The importance sampling has two motives: 1) original distribution p(z) is difficult to sample from, or 2) reduce estimation variance. (ref wikipedia.org)

\subsection{Noise contrastive estimation}

(used in word embedding?)

\subsection{Gibbs Sampling}


\subsection{Variational Inference}





\section {Approximate Inference}

\subsection{variational inference}

\subsubsection {Problem}
Variational Inference is related to calculus of variation. variational methods has nothing intrinsically approximate, but it provide ways to find approximate solutions.

log marginal probability
$$\ln p(X) = \mathcal{L}(q) + KL(q || p)$$
where
$$\mathcal{L}(q) = \int q(Z) ln{p(X,Y) \over {q(Z)}} dZ $$
$$KL(q \Vert p) = - \int q(Z) \ln \left \{ { p(Z \vert X)\over {q(Z)} } \right \} dZ$$

where p(X) is assumed difficult to optimize, and q(z) is an arbitrary distribution, 
$$\mathcal{L}(q) + KL(q \Vert p) = \int q(Z) ln(p(X)) dZ = \ln p(X) $$

because $KL(q \Vert p) \geq 0$, $\mathcal{L}(q)$ becomes the lower bound of $\ln p(X)$

VI interpreted with Expectation is easier to understand.

ref: 1601.00670.pdf

For latent variable z, data x, we need to compute $p(z|x)$, or p(x).

from a family of distribution L, we find the best one to approximate $p(z|x)$

$$q^*(z) = \arg \min_q KL(q(z) \Vert (p(z|x))$$

finally we approximate $p(z|x)$ with $q^*$

so variational inference turns the inference problem into an optimization problem.


relations to other domains:1)neural networks, 2)mcmc, 3)EM algorithm, 4)Gibbs sampler


basic ideas:
1)mean-field inference
2)coordinate-ascent optimization

problem

$$p(z|x) = {p(x,z) \over {p(x)}}$$

p(x) is the marginal density of the observations, called \textit{evidence}.
$$p(x) = \int p(x,z)dz$$

out goal if to find $q(z)$ the is close to $p(z|x)$,with respect to $KL(q|p)$

\begin{align}
KL(q(z)|p(z|x)) & = E_q[\log q(z)] - E_q[\log p(z|x)] \\
&= E_q[\log q(z)] - E_q[\log p(z,x) - \log p(x)]\\
&= E_q[\log q(z)] - E_q[\log p(z,x) + E_q[\log p(x)]\\
&= E_q[\log q(z)] - E_q[\log p(z,x)] + \log p(x)\\
\\
\log p(x) &= KL(q(x) \Vert p(z|x)) +  E_q[\log p(z,x)]  - E_q[\log q(z)]\\
&= KL(q(x) \Vert p(z|x)) + ELBO(q)
\end{align}

maximize Evidence Lower Bound (ELBO(q)) is equivalent to minimize KL divergence, since log q(x) is a constant w.r.t q(z).

\begin{align}
ELBO &= E_q[\log p(z,x)]  - E_q[\log q(z)] \\
&= E_q[\log p(x|z)] + E_q[\log p(z)] - E_q[\log q(z)] \\
&= E_q[\log p(x|z)] - KL[q(z) \Vert p(z)]
\end{align}

so maximize ELBO is to maximize log complete likelihood, and minimize KL divergence between prior probability p(z) and approximate prior q(z)

\subsubsection{mean filed inference}
latent variable z is factored into independent $\{z_j\}$, and 
$$q(z) = \prod_j q(z_j)$$
\begin{align}
ELBO(q) &= E_q[\log p(z,x)]  - E_q[\log q(z)] \\
        &= E_{q_j} [E_{q_{-j}} [\log p(z_j, z_{-j}, x)]] - E_{q_j} [E_{q_{-j}} [\log q(z)] ] \\
        &= E_{q_j} [\log {\exp (E_{q_{-j}} [\log p(z_j, z_{-j}, x)]) 
        	\over 
        	D }] 
        + \log D
        - E_{q_j}[\log q(z_j)] +C \\
        &= - KL_{q_j}(q(z_j) \Vert {exp([E_{q_{-j}} [\log p(z_j, z_{-j}, x)]]) \over D}) +C\\
        &= E_{q_j} [E_{q_{-j}} [\log p(z_j, z_{-j}, x)]] - E_{q_j}[\log q(z_j)] +C \\
        &= E_{q_j} [\ln \exp(E_{q_{-j}} [\log p(z_j, z_{-j}, x))]] - E_{q_j}[\log q(z_j)] +C \\
        &= - KL_{q_j}(q(z_j) \Vert {\exp([E_{q_{-j}} [\log p(z_j, z_{-j}, x)]]) \over D}) +C
\end{align}

D is used to normalize the  exp and then to make it a probability density of $z_j$. typically $$ D = \int exp([E_{q_{-j}} [\log p(z_j, z_{-j}, x)]]) dz_j$$

to maximize ELBO(q), the KL part should be 0, means
$$q_{z_j}^* \propto exp([E_{q_{-j}} [\log p(z_j, z_{-j}, x)]])$$

note $-j$ above means $\{i | i \neq j\}$

with above result, we can optimize ELBO iteratively w.r.t. $z_j$ for each j.

\subsection{Gibbs sampling}

\section {statistics for application}





\chapter{Math}

\section{SVD}   
SVD(singular value decomposition) has a relation to Lagrange multipliers theorem. Find the details.   
\begin{itemize}
\item they both uses $\lambda$
\item SVD can be proved using Lagrange Multipliers theorem, and Weierstrass theorem. (*topics in matrix analysis*, Horn Johnson).
\item Horn Johnson also discussed the related history.
\item *Vector calculus, linear algebra, and differential forms* also gives a brief history, in which the author says SVD is first proved by Lagrange, and the **eigenvalues are in fact Lagrange multipliers**. (but it seems that the author deleted this content in new edition of the book)
\end{itemize}

\subsection{Numerical methods to compute SVD}
\begin{itemize}
	\item common methods
	\item fast methods
	\item methods used by industry
	
\end{itemize}

papers to check:

Large-scale Parallel Collaborative Filtering for
the Netflix Prize

Finding Structure with Randomness:
Probabilistic Algorithms for
Constructing Approximate
Matrix Decompositions?

 Golub's and Van Loan's matrix computations

Å. Björck's Numerical Methods for Least Squares Problems

Nela Bosner
Fast Methods for Large Scale
Singular Value Decomposition
Doctoral Thesis

On Parallelizing Matrix Multiplication by the Column-Row Method?
Andrea Campagna? Konstantin Kutzkov? Rasmus Pagh§

links:

https://en.wikipedia.org/wiki/Principal\_component\_analysis


software:
 Eigen, Armadillo and Trilinos
 
 

\section{PCA}
what's the difference between SVD and PCA?
If we remove the mean from features before doing PCA, then PCA is just SVD.

(prove it, using PCA's correlation matrix)

\section{linear algebra}
for matrix A, f(x) = Ax, what is the max(f(x))? does it relate to eigen-value/eigen-vector of A? if A has rank(A) eigen-vectors, what's the relationship between these eigen-vectors and f(x)?

anti-symmetric matrix:
$-A = A^T$, 
for such matrix, $x^TAx = 0$

scalar matrix: dI is called scalar matrix, a scalar multiple of the identity matrix.

\subsection {Cholesky decomposition}



\begin{itemize}
	\item Freobenius norm
	
	sum of square of each elements
	
\end{itemize}
\section{Probability Theory}


	\subsection{Geometric distribution}
	$$P(k) = (1-p)^{k-1}*p$$
	\\
	the name maybe has something to do with geometric sequence: a sequence of numbers where each term after the first one is found by multiplying the previous one by a fixed, non-zero number called the common ratio.(wikipedia)   
	also named ????   
	in contrast, the arithmetic sequence:   
	a sequence of numbers where the difference between consecutive ones is constant (common difference).   
	????   
	\\
	Let P(0) = p, P(1) = 1-p, then P(k) means (k-1) 1's before a 0 occures.


	\subsection{Binomial distribution}
	$$P(k) = (^n_k)p^k(1-p)^{n-k}$$
	\\
	Let P(0) = p, P(1) = 1-p, then P(k) means k out of n times experiment outcome is 0.
	
	\subsection{Gaussian distribution (normal distribution)}
	$$N(x|\mu,\sigma) = P(x) = \frac 1 {\sqrt{2\pi}\sigma} exp(-\frac {(x-\mu)^2} {2\sigma^2})$$
	$\beta = 1/\sigma^2$ is called precision

	the sum of a random variables(\textbf{N terms}), which is itself a random variable, is Gaussian, as long as the \textbf{number of terms(N) in the sum} is large enough. (central limit theorem)
	
	\subsection{Gamma distribution}
	Gamma function is generalization of factorial $x!$, defined as 
	$$ \Gamma(\alpha) = \int_0^{\infty} x^{\alpha-1}e^{-x}dx $$
	
	for every positive integer n, $\Gamma(n) = (n-1)!$
	
	the times between successive occurrences in a Poisson process have an exponential distribution, which is a special case of Gamma distribution.
	
	
	\subsection{Exponential Distribution}
	
	$$
	f(x|\beta) =
	\begin{cases} 
	\beta e^{-\beta x} & x > 0 \\
	0 & x \leq 0 
	\end{cases}
	$$
	
	\subsection {random variable}
	Entropy
	$$H[x]= - \sum_{x} p(x)log_2p(x)$$
	
	Differential Entropy
	$$-\int p(x)lnp(x)dx $$
	
	The distribution that maximizes the differential entropy is the Gaussian (prove it)
		
	Cross Entropy
	$$H(p,q) = - \int_x p(x) log(q(x))$$
		
	\subsection{Random vectors}
	
	Probability density of a random vector, is the joint probability density of its components.
	
	\subsection {Gaussian random vectors}
	
	If $(x_1, x_2, ..., x_n)$ is a Gaussian random vector, then each element $x_i$ is a Gaussian random variable, and each subset from a Gaussian random vector, too. However, this relationship is not reversible, that is, if $x_1, x_2, ..., x_n$ are all Gaussian random variables, the vector $(x_1, x_2, ..., x_n)$ is not necessarily a Gaussian Random vector.
	
	The properties of Gaussian random vectors follow largely from the jointly Gaussian property rather than merely the property of being individually Gaussian.
	
	Problem: derive the joint probability density function of a Gaussian random vector. 
	
	$p(x|m, \Sigma) = (2 \pi )^{-D/2} *|\Sigma|^{1/2}* exp(-{\frac 1 2}(x-m)^T \Sigma^-1 (x-m))$

	Mode of Gaussian distribution

	\subsection{Concetps}
		\begin{itemize}
		\item variance
		
		$$\sigma^2 (X) =  \int_x p(x) (x - E[x])^2 = E[(x-E[x])^2]$$
		\item central limit theorem
		
		\item likelihood 
		
		Likelihood of parameter w, is the conditional probability of output given w, $p(t|w)$
		
		\item Mode of distribution
		
		The mode is a value that appears most often in a set of data, or the most probable value.
		For Gaussian, the mode equals to mean.
		
		\item What is a Moment Generating Function(MGF)?  
		
		
		In probability theory and statistics, MGF is another way to describe a random variable besides probability density function (pdf) or cumulative distribution function (cdf). However, notice that not all random variables have a MGF.
		
		
		MGF is defined as following:
		$$M_x(X) = E[e^{tX}], t \in R$$ whenever this expectation exists. 

		
		\item Moment
		
		from wikipedia: the points represent probability density, the zeroth moment is the total probability(one), the first moment is the mean, the second central moment is the variance, the third is the skewness, and fourth kurtosis.
		
		k-th \textbf{moment} is defined as expectation of $E[x^k]$
		
		k-th \textbf{central moment}: $E[(X-\mu)^k]$
		
		k-th \textbf{sample moment}: $$ {1 \over n}\sum_{j=1}^n {X_j^k} $$
		
		
		\end{itemize}
\section{Information Theory}
\begin{itemize}
	\item conditional entropy of X give Y:
	
	\begin{align*}
	H(X|Y) &= \sum_y H(X|Y=y) \\
		   &= \sum_y P(y) \left[ \sum_x P(x|y)\log{1 \over {p(x|y)}} \right]\\
		   &= \sum_{x,y} P(x,y)\log{1 \over {p(x|y)}}
	\end{align*}
	
	\item relative entropy, or Kullback-leibler divergence:
	
	\begin{align*}
	D_{KL}(P||Q) = \sum_x P(x) log{P(x) \over Q(x)}
	\end{align*}
	where P and Q are probablities over the same alphabet Ax.
	
	\item Gibb's inequality
	$$ D_{KL}(P||Q) >= 0$$
	
\end{itemize}
\section{Miscs} 
\begin{itemize}

	\item \textit{Moore-Penrose pseudo-inverse} of a matrix
	
	$(\phi^T\phi)^{-1}\phi^T$
	
	\item Gram matrix
	
	$\phi\phi^T$
	
	
	
\end{itemize}
	
	
\section{Multi-variable functions}
\begin{itemize}
	\item interior - x is interior to set A if there is some neighborhood U of x, such that $U \subset A$
	
	\item open set - every point of A is interior, then A is open.
	
	\item closed set - complement $A^c$ is open, then A is closed.
	
	\item compact set - in $E^n$, closed and bounded set. more general definition: a subset S of a topological space $S_0$ is compact if every open covering of S contains a finite sub-covering.
	
\end{itemize}
\subsection {Problems}
\begin{itemize}

	
	\item why topology define compact set in such a wield way?
	
	\item what are the problems that measure theory solved while other old theories can't solve?
\end{itemize}


\section{Vector Calculus}
\begin{itemize}
	\item gradient
	\item derivative of vector formulas
	
	$\frac {d(x^Tx)} {dx} = 2x$
	
	$\frac {d(x^TMx)} {dx} = (M+M^T)x$
	
	$\frac {d(Mx)^T(Mx)} {dx} = \frac {d(x^TM^TMx)} {dx} = 2(M^TM)x $
	
	$\frac {d(Mx-y)^T(Mx-y)} {dx} $\\
	$= \frac {d (X^TM^TMx - x^TM^Ty - y^TMx - y^Ty ) } {dx} $\\
	$= 2M^TMx -M^Ty - (y^TM)^T$\\
	$= 2M^T(Mx - y) $
\end{itemize} 
\section{Analysis}
\begin{itemize}
	\item $L^1$ space: integrable functions on $R^d$ with the norm defined as $$ ||f(x)|| = \int_{R^d} |f(x)|dx $$
	
	\item $L^2$ space: integrable functions on $R^d$ with the norm defined as $$ ||f(x)|| = \left( \int_{R^d} |f(x)|^2 dx \right) ^{1 \over 2} $$
	inner product defined as  $$<f,g> = \int f(x) \overline{g(x)} dx$$
	metric defined as $$d(f,g) = ||f-g||$$
	it's a hilbert space.
	
	\item {Quadratic forms}
	
	Quadratic forms are polynormials, all of whose terms are of degree 2. 
	
	quadratic forms as a sum of squares: all quadratic forms on $R^n$ can be decomposed into sums of m linearly independant linear functions. 
	
	$$Q(x) = (\alpha_1(x))^2 + ... + (\alpha_k(x))^2 - (\alpha_{k+1}(x))^2 ... - (\alpha_{k+l}(x))^2$$
	
	where $x \in R^n$
	
	and the number k and l are independant of specifically choosen linear functions, they depend only on Q(x).
	
	(k,l) is the so-called signature of the quadratic form $Q(x)$
	
	Q:how to use properties of quadratic forms to decide the type of a critical point? local mininum/maximum/saddle?
	
	
	\item Differential Forms
	\item inverse function theorem
	
	
	invertablity of derivative of f means invertablity of f locally, because derivative D(f) is very good local approximation for f.
	
	$F(u_0+h) = F(u_0) + D_{u_0}h + r(h)$
	
	where $r(h) = o(||h||)$ as $h \to 0$
	
	\item Banach fixed point theorem
	
	\item Questions
	
	The determinant of a matrix is the volume of the parallegram spanned by the vectors of the matrix, prove it.
	
	
	
\end{itemize}

\section{Topology}



\begin{itemize}
	
	\item definition 
	
	A set X with a topology T, is a collection of open subsets of X, that 1. $\emptyset$ and X is in T, 2. finite intersection of open sets is in T, 3. arbitrary union of open sets is in X.
	
	\item first-countable space
	
	A space X is said to be first-countable if each point has a countable neighbourhood basis (local base).
	
	\item second-countable space
	
	A space is said to be second-countable if its topology has a countable base. More explicitly, this means that a topological space  T is second countable if there exists some countable collection $ U=\{U_{i}\}_{i=1}^{\infty }$ of open subsets of T such that any open subset of  T can be written as a union of elements of some subfamily of U.
	
	\item hausdorff space
	
	A space X where all distinct points of X are pairwise neighborhood-seperable. If x, y are distinct points of X, there exists a neighbor U of x and a neighbor V of y such that $U \cap V = \emptyset $
	
	\item open
		'open' a predefined concept in topology, in other words, 'open' is not defined inside topology.
		
	how is open defined? or, is 'open' a predefined concept in topology, in other words, 'open' is not defined inside topology?
	
		
	\item closed
	
	if complement of a set O is open, then O is closed. note that closed is not the other direction of open, since some set are both open or closed, or are neither open nor closed.

	
	closure
	
	bound - $\partial S = closure(S) - interior(S)$
	
	continuous map - inverse image of open set is open. in metric spaces, continuous functions is defined to be those whose image at a point y changes arbitrary small  given sufficient small change of input point x. formally, f is continuous at point c of its domain if, for any neighborhood N1(f(c)), there is a neighborhood N2(c), such that  f(x) is in N1 whenever x is in N2.  using this definition, it can be proved that the definition of continuous function in topology and in metric spaces are equivalent. [P.-A. Absil] defined it whis way, a basis is a collection of subsets of X, such that 1) any x in X is in at least one elements, 2) if x is in both subset a and subset b then there must be subset c which is subset of intersection of a and b, such that x is in c. it seems that the latter is more popular, Wolfram uses this def. but, are they equivalent?
	
	homeomorphism - continuous bijective map with continuous inverse
	
	local homeomorphism - every point $x \in X$ has a neighborhood $U \subset X$ such that f(U) is a open subset of Y, and f restricted to U is homeomorphism.
	
	totological property - invariant under homoemorphism. e.g. size is not a topological property, but connectedness, path connectedness, compactness are.
	
	basis of a topology - basis may be defined differently by authors, [Jonh M. Lee] defined it as a collection of some open subsets, such that any open subset is a union of some elements of it. 
	
	cover - a collection A of subsets of X, covers X, if for any point p of X, there exists a U in A such that p is in U.
	
	open cover - each of the sets is open
	
	closed cover - each of the sets is closed.
	
	subcover - given cover U, $U' \subset U$ is also a cover, then U' is a subcover.
	
	compactness - every open cover has a finite subcover. the compact set is defined in metric spaces as bounded and closed, Heine?Borel theorem says that the two definitions are equivalent.
	
	dense - a set A is said to be dense in X if $\overline{A}=X$
	limit point compact
	
	sequentially compact
	
	local compactness
	
	connectedness
	
	path connected
	
	complete
	
	precompact
	
	paracompact
	
	Cauchy sequence
	
	dense subset
	
	product topology of $X_1, X_2$ ... - $\boldmath{B} = \{U_1 \times U_2 ...\}$, where $U_i$ is open subsets of $X_i$
	
	subspace topology
	
	quotient topology
	
	disjoint topolody
	
	adjunct spaces - X and Y are topology spaces, A is closed subspace of Y, function $f: A \to X$ defines a equivalence relation $a eq f(a)$.
	the quotient space of $X \bigsqcup Y$, by f, is called attaching Y into X along f. any $f^{-1}(x)$ is equivalent to $a \in A$.
	
	topological groups
	
	locally finite: a collection A of subsets of X, is locally finite, if for all x in X, x has a neighborhood that intersect at most finitely many of sets in A. (lie: if A is finite, A is locally finite. infinite collections can also be locally finite, e.g. $A=\{(n,n+2)|n \in Z \}$, a countable collection of subsets need not be locally finite, e.g. (-n, n) with integer n. But no infinite collections of a compact space can be locally finite.)
	 
	partitions of unity
	
	paracompact: a space X is paracompact if every open cover of X admits a locally finite open refinement. every compact space is paracompact, because a finite subcover is a locally finite open refinement. A monifold is paracompact because it's second countable (countable basis)
	
	paracompactness theorem: every second countable, locally compact housdorf topological space is paracompact.
	
	normal space - disjoint closed subsets can be seperated by open subsets.
	
	regular space - closed subset and point not in this subset can be seperated by open subsets.
	
	normal => regular => Housdorff
	
	
	classification thoerem: every nonempty connected 1-manifold is homeomorphic to $R$ or $S^1$
	
	closed map:  takes closed subsets of X to closed subsets of Y.
	
	open map: takes open subsets of X to open subsets of Y.
	
	CW complexes, named after  J. H. C. Whitehead
	
	simplicial complexes
	
	$\mathbb{P}^{n}$ - real projective space of dimension n, set of one dimentional linear subspaces(lines through origin) in $\mathbb{R}^{n+1}$
	
	$\mathbb{P}^{2}$ - projective plane
	
	fundamental group
	
	homotopy - funtion f is homotopic to g, if continuous map H(x,t) exists that H(x,0)=f, h(x,1)=g. H deforms f to g continuously. 
	homopotic relative to A if for all a in A, H(a,t)=f(a)=g(a).
	freely homotopic if A is empty. a path homotopy is a homotopy that is stationary at the end points.
	
	path class is the path homotopy equivalence class.
	
	fundamental group of X based at p, is the set of path classes of loops based at p, denoted by $\pi_1(X, p)$.
	
	simply connnected, means connnected, and every loop can be shrunk to  a constant loop. while its base point is fixeds.
	
	homomorphism: structure preserving map.
	
	retraction: if $A \subset X$, a continuous map $r: X \rightarrow A$ restricted on A is $id_A$, then r is a retraction, and A is a retract of X.
	
	a retract of a simply connected space is simply connected. this theorem can be used to check if a space is simply connected or not. we first find a retract, and if the retract is not simply connected, then we know the original space is not simply connected.
	
	topological invariance of $\pi_1$. homeomorphic spaces have isomorphic fundamental groups. specifically, if $\phi$ is homeomorphism, then $\phi_*: \pi_1(X, p) \rightarrow \pi_1(Y, \phi(p))$ is an isomorphism.
	
	topological group: a group endowed with a topology such that the multiplicatoin and inverse operation are continuous. every topological group is topologically homogeneous, that is, given any two points in the group, there is a homeomorphism that take one to the other. They look the same from the vantage of any point.
	
	discrete topology - given any set X, let the power set of X (P(X)) be the topology, so every subset of X is open.  this is called the discrete topology on X.
	
	trivial group is a group with only the identity element.
	
	fundamental group of a circle, $\pi_1(\mathbb{S}^1, 1)$ is an infinitly cyclic group generated by $\omega(s) = e^{2\pi i s}$
	
	degree - degree of a continuous map $\phi : \mathbb{S}^1 -> \mathbb{S}^1$ if the winding number of $\phi \circ \omega$, and $\omega : I -> \mathbb{S}^1$ is the standard generator of $\pi_1(S^1, 1)$.  it's denoted by $deg \phi$
\end{itemize}


\section {Matrix Manifold}
\begin{itemize}
	\item chart
	
	A bijection $\phi$ of a subset $U$ of $M$ onto an open subset of $R^d$ is called a d-dimentional chart of set M, denoted by  $(U, \phi)$
	
	\item {atlas}
	
	An atlas of $M$ into $R^d$ is a collections of charts $(U_{\alpha}, \phi_{\alpha })$ of $M$ such that 
	
	1) $\bigcup_{\alpha}U_{\alpha} = M$\\
	2) the elements of an atlas overlap smoothly. \\
	if $U_{\alpha} \cup U_{\beta}  \neq \emptyset$, the sets $\phi_{\alpha}(U_{\alpha} \cap U_{\beta})$ and  $\phi_{\beta}(U_{\alpha} \cap U_{\beta})$ are open sets of $R^d$ and the changes of coordinates $\phi_{\beta} \circ \phi_{\alpha}^{-1} : R^d \to R^d $ is smooth.
	
	\item {manifold}
	
	a (d-dimensional) manifold is a couple $(M, A^+)$, where $M$ is a set and $A^+$ is a maximal atlas of $M$ into $R^d$, such that the topology induced by $A^+$ is hausdorff and second-countable.
	
	lie: this definition is different from the one defined in analysis fields. In those literatures a manifold is a topological space that locally resembles euclidean space near each point. Maybe the only defference is the set M.
	
	
	\item embedded submanifolds
	\item quotient manifolds
	\item generalized eigen-value problem
	
	
	finding eigen pairs of matrix pencil is known as the generalized eigen-value problem. $Av = \lambda B v$ where $(A,B)$ is a matrix pencil.
	
	\item {Stiefel manifold}
	
	$\{M:R^{n*p}\}$ where all columns of M are linearly independant, and $p \leq n$, is called noncompact Stiefel manifold of full-rand n*p matrices.
	
	
	$\{X \in R^{n*p}, X^T X = I_p\}$ is compact Stiefel manifold. 
	
	
	\item {quotient manifold}
	
	the set of equivalent classes of a relation r of M is called the quotient of M by ~, denoted as $M/r$. 
	
	\item {problems}
	
	is it possible to use category theory to interpret some topics in matrix manifold?
	
\end{itemize}


\section {differential geometry}
Homeomorphism: continuous function between topological spaces, that has a continuous inverse function.

Diffeomorphism: smooth function that has a smooth inverse function. diffeomorphism is by definition a homeomorphism.

For a manifold M, a map from U to M is called parametrization, and the inverse map is called coorindation.

a manifold has dimension zero if each point p of m has a neighbor containing p alone.

Inverse Function Theorem. If the derivative $  df(x) : R^k \to R^k$ is nonsingular,
then $f$ maps any sufficiently small open set U' about x diffeomorphically onto an open set $f(U')$

Question: why is the inverse function theorem the fundation of differential geometry?

 fields
  
 vector fields: imagine every point is a vector.
 
 bundle: a triple $(E, \pi, M)$, E is a smooth manifold called total space, M is smooth manifold called base space. $\pi$ is smooth map that is surjective, called projection map.
 
 fibre: for a bundle $(E, \pi, M)$,  $p \in M$, fibre over p is the preimage of p over $\pi$
 
 section: a section of a bundle, (E, $\pi$, M), a section is map $\sigma$ from base space to total space, (the other way from $\pi$), that $\pi \circ \sigma = id_M$
 
 
 tangent bundle of smooth manifold
 
 (M,O,d) a smooth mandifold, TM := disjoint union of all TpM.
\chapter{Problems}

Generative model: $p(y|x) = p(x|y)p(y)/p(x)$, models both input and output distributions explicitly or implicitly.


Discriminative model: $p(y|x)$
models posterior distribution directly.


\chapter{NLP}

\section{Word Vectors}

\subsection{Skip Gram model}
use centor word to predict context words, maximize the average log probability
$1\over T \sum$

$$ p(w_o | w_c) = {exp(<w_o,w_c>) \over \sum_{j \in V} exp(<w_j,w_c>) })$$

cost function:
\begin{itemize}
	\item full softmax
	\item hierarchical softmax
	\item NCE(noise contrastive estimation)
	\item NEG(negative samping)
	$$log(\sigma <wo,wi>) + \sum E_{w_i  p_n(w)} {log(\sigma (- <wo,wi>))}$$
\end{itemize}


\subsection{CBOW model}

CBOW - use context words to predict center word, with the context words represented as a summary of word vectors.

\subsection{GloVe}
GloVe

\chapter{Reinforcement learning}


\section{Concepts}
\begin{itemize}
	\item General policy iteration
	
	value function is repeatedly altered to more closely approximate the value function of the current policy, and the policy is repeated improved with respect to current value function. These two processes intermix as each creates a moving target for the other.
	
	\item Bellman equality
	\item Bellman optimal equation
	\item Dynamic programming
	\item on-policy vs off-policy
	
	Off-policy prediction refers to learning the value function of a target policy from data generated by a different behavior policy.
	
	\item off-policy learning
	
	The policy being learned
	about is called the target policy, and the policy used to generate behavior is called the behavior policy.
	In this case we say that learning is from data "off" the target policy, and the overall process is termed
	off-policy learning.
	
	\item TD(0)

	\item Monte-Carlo
	
	for Monte-Carlo prediction of state values, just generate full episodes, average returns as state values.
	
	for action values, use the same method as for state values.
	\item Importance sampling
	
	use importance sampling to learn off-policy.
	
	
	\item Q-learning
	\item Sarsa
	\item sarsa expected
	\item n-step bootstrapping
	
	n-step bootstrapping unifies 1-stop TD learning (one end) and episode monte-carlo learning (the other end). It just uses N-step state/action reward to update value function.
	
	\item maximization bias
	\item Double Q learning
	
	\item prediction with approximation
	
	in reinforcement learning, the target is non-stationary (change over time), compared to common supervised learning where the target is static.
	
	\textbf{prediction objective}
	$$J(w) = \overline{VE}(w) = \sum_{s\in S} \mu(s) [\hat v(s,w) - v_{\pi}(s)]^2$$
	where $\mu$ is used to control importance of state s, $\sum_s \mu(s) = 1$. in practice $\mu(s)$ is often the fraction of time spent in s. Under on-policy training it's called on-policy distribution. 
	
	\textbf{improve w}
	$$w_{t+1} = w_t - \alpha \mu(s)(\hat v(s,w) - v_{\pi}(s)) \nabla \hat v(s,w) $$
	
	\textbf{target value}
	
	target value is often approximated as in bellman-equation.
	In TD(0), $v_{\pi}(s) = R+\gamma v_{\pi}(s')$
	
	
	\item $\lambda$ - Return
	$$G_t^{\lambda} = (1 - \lambda) \sum_{n=1}^\infty \lambda ^ {(n-1)} G_{t:t+n}$$
	where
	$$G_{t:t+n} = \sum_{n=1}^{n} \gamma^{n-1}R_t + \gamma^{n} \hat{v}(s_{t+n}, w_{t+n-1})$$
	
	$\lambda$-return is the average of all n-step backups.
	
	\item offline $\lambda$-return algorithm
	
	$$w_{t+1} = w_t + \alpha (G_t^{\lambda} - \hat{v}(s_t,w_t)) \nabla \hat{v} (s_t, w_t)$$
	
	note that it is value function approximation.
	
	\item Eligibility traces
	
	\item TD($\lambda$)
	\begin{align}
	z_{t} &= \lambda \gamma z_{t-1} +   \nabla \hat{v}(s_t, w_t) \\
	w_{t+1} &= w_t + \alpha \delta z_t \\
	\delta &= R_{t+1} + \gamma \hat{v}(s_{t+1},w_t) - \hat{v}(s_t, w_t)	
	\end{align}
	
	$$w_{t+1} = w_t + \alpha (R_{t+1} + \gamma \hat{v}(s_{t+1},w_t) - \hat{v}(s_t, w_t)) z_t \\
	$$
	
	z is eligibility trace. compared to offline $\lambda$ return, the $\delta$ is replace by 1-step time difference, and $\nabla \hat{v}$ is replaced by eligibility trace z, z contains both current and history gradient.
	
	Why is it called a time-backward algorithm? check the updating mechanism of w, old time gradient have chances to multiply the current TD error! although we compute it forward, it has backward effects, with respect to w updating.
	
	It's kind of like convolution.
	
	\item Policy gradient
	\item Policy gradient theorem
	
	This theorem says that policy gradient is independant of the state distribution. (Independant of the model dynamics.) Reinforcement Learning: An introduction gives an proof for episodidc case, but a much easy proof is provided in Pieter Abbeel's lecture (Youtube). refer to likelihood ratio policy gradient.
	
	
	$$\nabla J(\theta) = \propto \sum_s \mu(s) \sum_a q_{\pi}(s,a) \nabla_{\theta} \pi(a | s, \theta)$$
	
	\item Difference between policy gradient and function approximation
	
	\begin{enumerate}
		\item Q/V value function approximation uses 'state distribution ($\mu$)' while PD doesn't.
		\item PD is actually gradient ascent
		\item FD iterate for each step, PD?
	\end{enumerate}
	
	\item Actor critic
	\item Deep Q learning
	
	\item PPO (Proximal Policy Optimization)
	
	https://blog.openai.com/openai-baselines-ppo/
	
	\item PPO2
	
	ppo2 is gpu enbaled ppo
	
	
	
	\item VPG
	\item TRPO
	
	https://arxiv.org/abs/1502.05477
	
	\item ACER
	
	https://arxiv.org/abs/1611.01224
	
	\item tensorforce
	\item OpenAI gym
	\item Deepmind lab
	
	\item Retrace algorithm
	
	
\end{itemize}

\section {Derivative free methods}

cross entropy methods\\
finite differences\\
fixing random seeds

\section {Cross Entropy Methods (CEM)}

It's also called evolutionary methods.

The name of cross entropy methods comes from the cross-entropy (KL) distance. It has two phase:

1) generate a sample of random data

2) update the parameters of the random mechanism, on the basis of the data, in order to produce a better sample in next iteration.

For policy improventment, the target function is

$$\max_{\theta} U(\theta) = \max_{\theta}E[\sum_{t=0}^H R(s_t) | \pi_{\theta}]$$

where $\pi_{\theta}$  is our policy.

CEM views U as a black box, ignores all other information other than U collected during episode.

population $P_{\mu^i}(\theta) $

pseudo code for CEM

\noindent \indent for iter = 1,2...\\
\indent \indent for population member e=1,2,...\\
\indent \indent \indent \indent sample $\theta^e \sim P_{\mu}(\theta)$\\
\indent \indent \indent \indent execute roll-outs under $\pi_{\theta^e}$\\
\indent \indent \indent \indent  store ($\theta^e, U(e)$)\\
\indent \indent end for\\
\indent \indent $\mu^{i+1} = \arg\max_{\mu} \sum_{\tilde{e}} log P_{\mu}(\theta^{\tilde{e}})$\\
\indent \indent where $\tilde{e}$ index over top p\% e \\
\indent end for\\

So this alg is searching $\theta$, which is parameterized by $\mu$, $\mu$ controls the distribution of $\theta$. at the end, we get a list of $\theta$ and corresponding $U(\theta)$. we choose the best $\theta$? and we have to model $\theta$ by parameter $\mu$?

simple, need full episodes.
caveats: parameters cannot be too many!




\section {Policy Gradient}


Performance measure

$$J(\theta)$$
J need to be differentiable w.r.t $\theta$

for not-too-large discrete action space, use softmax to select action. each action has a parameterized preference value as $h(s,a,\theta)$
$$\pi(a|s,\theta) = {{exp(h(a,s,\theta))}\over{\sum_b{exp(h(b,s,\theta))}}}$$

if we use a linear apporximation for h, it's
$$h(s,a,\theta) = \theta^Tx(s,a)$$
where x is the feature vector.

by above formula, we are modeling action selection probability distribution.

advantage of policy gradient: selecting actions according to softmax can approach a deterministic policy, while with $\epsilon$-greedy selection by action values there is always a probability of $\epsilon$ selecting random values.
although we can use softmax of action values but that alone cannot allow the policy to approach deterministic, because action or state values is designed to approach real action/state value, this make them impossible to be 1 or 0 as of the probability.

Define $J(\theta) = v_{\pi_{\theta}}(s_0)$, policy gradient theorm tells 

\begin{align}
\nabla J(\theta) &= \propto \sum_s \mu(s) \sum_a q_{\pi}(s,a) \nabla_{\theta} \pi(a | s, \theta) \\
&= E_{\pi}[\sum_a q_{\pi}(s,a) \nabla_{\theta} \pi(a | s, \theta)] \\
&= E_{\pi}[\sum_a \pi(a|s, \theta) q_{\pi}(s,a) {\nabla_{\theta} \pi(a | s, \theta) \over {\pi(a|s, \theta)}}] \\
&= E_{\pi} [G_t {\nabla_{\theta} \ln \pi(A_t | S_t, \theta)}]
\end{align}

this formula gives the REINFORCE algorithm. for each episolde, for each step, 
$$\theta = \theta + \alpha G_t {\nabla_{\theta} \ln \pi(A_t | S_t, \theta)}$$

$\nabla_{\theta} \ln \pi(A_t | S_t, \theta)$ is called \textit{eligibility vector}.

\subsection{policy gradient with baseline}

$G_t$ is replaced with $G_t - b$ in REINFORCE algorithm, the baseline b can be used to reduce variance of policy update.

\subsection{Finite difference methods}
\subsection {Likelihood ratio methods}



let $\tau$ be the trajectory, a action sequence $s_0,u_0, s_1, u_1,...$, and $R(\tau) = \sum_{t=0}^H R(s_t,u_t)$ be total rewards.
$$U(\theta) = E[\sum_{t=0}^H R(s_t, u_t); \pi_{\theta}] = \sum_{\tau}P(\tau, \theta)R(\tau)$$

our goal is to find $\theta$:
$$\max_{\theta}U_{\theta} = \max_{\theta} \sum_{\tau}P(\tau, \theta)R(\tau) $$

the gradient of U is :
\begin{align}
\nabla_{\theta}U &= \sum_{\tau}\nabla_{\theta}P(\tau, \theta)R(\tau)\\
&= \sum_{\tau} P(\tau; \theta) {\nabla_{\theta}P(\tau, \theta) \over P(\tau, \theta) }R(\tau)\\
&= \sum_{\tau} P(\tau; \theta) {\nabla_{\theta}log P(\tau, \theta)}R(\tau)\\
&= E_{\tau,\theta}[{\nabla_{\theta}log P(\tau, \theta)}R(\tau)]
\end{align}

it's possible to derive this equation from importance sampling view:

\begin{align}
U(\theta) &= E_{\tau \sim \theta_{old}} 
[{P (\tau | \theta) 
	\over 
	{P(\tau | \theta_{old})}}
R(\tau)]\\
\nabla_{\theta} U(\theta) &= E_{\tau \sim \theta_{old}} 
[{\nabla_{\theta} P (\tau | \theta) 
	\over 
	{P(\tau | \theta_{old})}}
R(\tau)]\\
\nabla_{\theta} U(\theta) |_{\theta = \theta_{old}} &= E_{\tau \sim \theta_{old}} 
[{\nabla_{\theta} P (\tau | \theta_{old}) 
	\over 
	{P(\tau | \theta_{old})}}
R(\tau)] \\
&= E_{\tau \sim \theta_{old}} 
[\nabla_{\theta} log P (\tau | \theta_{old}) R(\tau)]
\end{align}

\subsubsection{vanilla policy gradient (VPG)}
\subsubsection{Natural policy gradient (NPG)}

\subsubsection{Trust Region Policy Gradient (TRPO)}

lte $\eta(\pi) $ be accumulated reward, 
$$\eta(\pi) = E_{s_0,a_0,...}\sum_{t=0}^{\infty} \gamma^t r(s_t)]$$


\section {Actor-Critic}

\chapter {Numerical analysis}

In numerical analysis, the 4 basic operations (+-*/) cause no problem except the subtraction (-), where cancellation causes round error to be significant, compared to operation result.

More tricks here.

\chapter {Deep Learning}

\section{Batch Normalization}

Effects: 
1) faster to train 2) less dependant on parameter initialization 3) regularization  effect, 4) remove the need of dropout


Problems to solve: 
initial convariat shifting - when parameters of previous lays are changed, their ouput changes also, and this means the inputs of later layers changes.

Solution:
normalize input for each layer.

Implementation:\\
1) Insert BN layer before or after activation, Ng says usually before activation. So does ResNet.

2) At inference there is often only one sample, and in no way to calculate mean or standard diviation. So they are computed while training, with exponential decayed averaging. 

3) in [deeping learning], the author says the major innovation is that "we back-propogate through these operations for computing the mean and the standard deviation, and for applying them to normalize H. This meas the gradient will never propose an operation that acts simply to increase the standard deviation and the mean of $h_i$, the normalization operations remove the effect of such an action and zero out its component in the gradient". not completely undersand this. It seems that the origial paper describes a different training procedure than implementation by keras or tensorflow. should try to make clear.


1) how is batch norm applied to conv-layer?\\
according to the original paper, it's applied to one feature map. i.e. for batch size n, and feature p*q, the actural batch size is n*p*q.\\
for some tasks, such as rcnn, where the batch size is small, e.g. 2 or 4 (since the model is very large and memory hungry), in this situation, the BN may cause errors unacceptable.


2) what are those batch-norm variaties? any advantages? group normalization, layer normalization.




\section{Object Detection and Segmentation}
\subsection{RCNN series}
\begin{itemize}
	\item RCNN
	
	\item fast-RCNN
	
	Feature reused before generating class ids and bounding boxes.
	
	\item faster-RCNN
	
	RPN is proposed to generate ROIs.
	
	\item Mask RCNN
	
	A third head branch is added to generate instance masks. \\
	ROI-Align replaced ROI-Pooling 
	
\end{itemize}


\chapter{Miscs}
\section{t-SNE}
t-SNE, full name t-distributed stochastic neighbor embedding, t-distributed means student t distribution. it's an improvement of SNE. So what is SNE? SNE maps high-dimensional data into low dimensional space by reducing KL divergence of distributions of data points in high and low dimensions. For any point i, consider the similarities with all other points as conditional probabilities $P(j|i)$, then we get a distribution of all data points except i. The actual probability is measured as function of pair-distances. 
$P(j|i) = {{\exp(-|x_i-x_j|^2 / {\sigma^2})} 
          \over 
          {\sum_{k\neq i}\exp(-|x_i-x_k|^2 / {\sigma^2})}}$
For distribution on low-dimensional space, it's almost the same except that no $\sigma$ is involved. we denote it as $Q(j|i)$. the cost function is $\sum_i KL(P_i||Q_i)$. With iterative gradient descent methods, we find a representation in low dimension space. There are other details such as how to decide $\sigma$, refer to original paper.  
SNE's cost function is non-convex, so in the original paper, a noise is added into gradient, to get effect of simulated annealing, it is hard to train, says the paper. the cost function looks a little complicated, but it's gradient is surprisingly simple. Choosing KL is not just a random decision, because KL is asymmetric and helps keep local structure in low dimension space. 

Then comes t-SNE, 2 improvement, 1)symmetric SNE, 2) t-distribution. By symmetrical SNE a  joint probability $p(i,j)$ is used instead of conditional probability $p(j|i)$. $p(ij) = \exp(|x_i-x_j|^2) / \sum_{k\neq l} \exp(|x_k-x_l|^2)$, by this means all data point pairs constitute a total big distribution, and a single KL divergence is used as total cost. This reduces the complexities of the algorithm. But in original high dimension space, the joint distribution has a problem for outliers, all the pairs related to the out-lier get very low probability, and contributes little to total cost and then their positions is not going to be well determined. so in t-SNE, $[p(i|j) + p(j|i)]/2n$ is used, while in low dimension space, the joint probability is used. the gradient is simpler than SNE. To alleviate the crowding problem, a student-t distribution is used in low-dimension space, (in high dimension space, Gaussian is used). student-t distribution has heavier tails, degree 1 is used.



reducing the tendency to crowd points together.

can be implemented via Barnes-Hut approximation

\end{document}